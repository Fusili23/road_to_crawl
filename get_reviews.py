import time
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from selenium.common.exceptions import NoSuchElementException
import pandas as pd
import sys
import os
from datetime import datetime

# CSS ì„ íƒì ìƒìˆ˜
REVIEW_TAB_SELECTOR = "a[href='#sdpReview']"
PAGE_BUTTON_SELECTOR = "button.sdp-review__article__page__num"
NEXT_GROUP_BUTTON_SELECTOR = "button.sdp-review__article__page__next:not([disabled])"
REVIEW_ARTICLE_SELECTOR = "sdp-review__article__list"
REVIEW_DATE_CLASS = "sdp-review__article__list__info__product-info__reg-date"
REVIEW_CONTENT_CLASS = "sdp-review__article__list__review__content"

# ê¸°ë³¸ê°’ ìƒìˆ˜
DEFAULT_PRODUCT_ID = "-1"
DEFAULT_DATE = "-1"
WAIT_TIMEOUT = 15
MAX_PAGE_LIMIT = 150  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì œí•œ


def safe_driver_quit(driver):
    """ë“œë¼ì´ë²„ë¥¼ ì•ˆì „ ì¢…ë£Œ"""
    if driver:
        try:
            driver.close()
            driver.quit()
        except Exception:
            pass


def extract_product_id(product_url):
    """URLì—ì„œ ìƒí’ˆ IDë¥¼ ì¶”ì¶œ"""
    try:
        url_parts = product_url.split('/products/')
        if len(url_parts) > 1:
            raw_product_id = url_parts[-1].split('?')[0].split('/')[0].strip()
            
            # ID ìœ íš¨ì„± ê²€ì¦
            if raw_product_id and (raw_product_id.replace('-', '').isdigit() or len(raw_product_id) > 3):
                return raw_product_id
            
        return DEFAULT_PRODUCT_ID
    except Exception as e:
        print(f"Product ID ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return DEFAULT_PRODUCT_ID


def create_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ìƒì„±"""
    options = uc.ChromeOptions()
    options.add_argument('--blink-settings=imagesEnabled=false')  # ì´ë¯¸ì§€ ë¹„í™œì„±í™” -> ì†ë„ í–¥ìƒ
    options.add_argument('--disable-dev-shm-usage')               # ë©”ëª¨ë¦¬ ê´€ë ¨ ì˜¤ë¥˜ ë°©ì§€
    options.add_argument('--no-sandbox')                          # ê¶Œí•œ ê´€ë ¨ ì˜¤ë¥˜ ë°©ì§€
    
    return uc.Chrome(options=options, use_subprocess=True)


def click_review_tab(driver):
    """ë¦¬ë·° íƒ­ í´ë¦­"""
    wait = WebDriverWait(driver, WAIT_TIMEOUT)
    review_tab = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, REVIEW_TAB_SELECTOR)))
    driver.execute_script("arguments[0].click();", review_tab)
    time.sleep(0.5)


def get_page_numbers_in_current_group(driver):
    """í˜„ì¬ í˜ì´ì§€ ê·¸ë£¹ì˜ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ"""
    wait = WebDriverWait(driver, WAIT_TIMEOUT)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, PAGE_BUTTON_SELECTOR)))
    
    page_buttons = driver.find_elements(By.CSS_SELECTOR, PAGE_BUTTON_SELECTOR)
    return [button.get_attribute("data-page") for button in page_buttons]


def navigate_to_page(driver, page_number):
    """íŠ¹ì • í˜ì´ì§€ë¡œ ì´ë™"""
    page_buttons = driver.find_elements(By.CSS_SELECTOR, f"button[data-page='{page_number}']")
    
    if not page_buttons:
        print(f"{page_number}í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í˜„ì¬ í˜ì´ì§€ ê·¸ë£¹ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return False
    
    current_page_button = page_buttons[0]
    
    # ì´ë¯¸ í™œì„±í™”ëœ í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ í´ë¦­
    if 'sdp-review__article__page__num--active' not in current_page_button.get_attribute('class'):
        driver.execute_script("arguments[0].click();", current_page_button)
        time.sleep(0.5)
    
    return True


def extract_reviews_from_current_page(driver, product_id):
    """í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ë¦¬ë·° ì¶”ì¶œ"""
    wait = WebDriverWait(driver, WAIT_TIMEOUT)
    wait.until(EC.presence_of_element_located((By.CLASS_NAME, REVIEW_ARTICLE_SELECTOR)))
    
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'html.parser')
    
    review_articles = soup.find_all('article', class_=REVIEW_ARTICLE_SELECTOR)
    reviews_on_page = []
    
    for article in review_articles:
        # ë¦¬ë·° ë‚ ì§œ ì¶”ì¶œ
        date_element = article.find('div', class_=REVIEW_DATE_CLASS)
        review_date = date_element.get_text(strip=True) if date_element else DEFAULT_DATE
        
        # ë¦¬ë·° ë‚´ìš© ì¶”ì¶œ
        content_element = article.find('div', class_=REVIEW_CONTENT_CLASS)
        review_content = content_element.get_text(strip=True) if content_element else ""
        
        # ìœ íš¨í•œ ë¦¬ë·°ë§Œ ì¶”ê°€
        if review_content:
            reviews_on_page.append({
                'ProductID': product_id,
                'Date': review_date,
                'Review': review_content
            })
    
    return reviews_on_page


def has_next_page_group(driver):
    """ë‹¤ìŒ í˜ì´ì§€ ê·¸ë£¹ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì´ë™"""
    try:
        next_group_button = driver.find_element(By.CSS_SELECTOR, NEXT_GROUP_BUTTON_SELECTOR)
        print("ë‹¤ìŒ í˜ì´ì§€ ê·¸ë£¹ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤...")
        driver.execute_script("arguments[0].click();", next_group_button)
        time.sleep(0.5)
        return True
    except NoSuchElementException:
        print("ë§ˆì§€ë§‰ í˜ì´ì§€ ê·¸ë£¹ì…ë‹ˆë‹¤. ìƒí’ˆ ë¦¬ë·° ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return False



def read_url_list(file_path):
    """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ URL ëª©ë¡ì„ ì½ì–´ì˜´"""
    if not os.path.exists(file_path):
        print(f"{file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"{file_path} íŒŒì¼ì„ ìƒì„±í•˜ê³  ì¿ íŒ¡ ìƒí’ˆ URLì„ í•œ ì¤„ì”© ì‘ì„±í•´ì£¼ì„¸ìš”.")
        return []
    
    urls = []
    line_number = 1
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    if "coupang.com" in line:
                        clean_url = line.split('?')[0]  # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
                        urls.append(clean_url)
                    else:
                        print(f"ê²½ê³ : ì¤„ {line_number} - ìœ íš¨í•˜ì§€ ì•Šì€ ì¿ íŒ¡ URLì„ ê±´ë„ˆëœë‹ˆë‹¤: {line}")
                line_number += 1
    except Exception as e:
        print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return []
    
    return urls


def process_single_product(product_url, product_index, total_products, driver):
    """ë‹¨ì¼ ìƒí’ˆì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘"""
    print(f"\n{'='*60}")
    print(f"[{product_index}/{total_products}] ìƒí’ˆ ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘")
    print(f"URL: {product_url}")
    print(f"{'='*60}")
    
    try:
        # ìƒí’ˆ í˜ì´ì§€ë¡œ ì´ë™
        print(f"ìƒí’ˆ í˜ì´ì§€ì— ì ‘ì†í•©ë‹ˆë‹¤...")
        driver.get(product_url)
        WebDriverWait(driver, WAIT_TIMEOUT)
        time.sleep(1)
        
        # ìƒí’ˆ ID ì¶”ì¶œ
        product_id = extract_product_id(product_url)
        print(f"Product ID: {product_id}")
        
        # ë¦¬ë·° íƒ­ìœ¼ë¡œ ì´ë™
        click_review_tab(driver)
        
        all_reviews = []
        
        # ëª¨ë“  í˜ì´ì§€ ê·¸ë£¹ ìˆœíšŒ
        while True:
            page_numbers = get_page_numbers_in_current_group(driver)
            
            # í˜„ì¬ ê·¸ë£¹ì˜ ê° í˜ì´ì§€ ìˆœíšŒ
            for page_number in page_numbers:
                try:
                    # ìµœëŒ€ í˜ì´ì§€ í™•ì¸
                    if int(page_number) >= MAX_PAGE_LIMIT:
                        print(f"ìµœëŒ€ í˜ì´ì§€ ì œí•œ({MAX_PAGE_LIMIT})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ìƒí’ˆìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                        return pd.DataFrame(all_reviews) if all_reviews else pd.DataFrame(columns=['ProductID', 'Date', 'Review'])
                    
                    # í˜ì´ì§€ ì´ë™
                    if not navigate_to_page(driver, page_number):
                        break
                    
                    print(f"  -> {page_number}í˜ì´ì§€ ë¦¬ë·° ìˆ˜ì§‘ ì¤‘...")
                    
                    # í˜„ì¬ í˜ì´ì§€ì˜ ë¦¬ë·° ìˆ˜ì§‘
                    reviews_on_page = extract_reviews_from_current_page(driver, product_id)
                    all_reviews.extend(reviews_on_page)
                    
                except KeyboardInterrupt:
                    print("\nìˆ˜ì§‘ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    raise
                except Exception as e:
                    print(f"  -> {page_number}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
            
            # ë‹¤ìŒ í˜ì´ì§€ ê·¸ë£¹ í™•ì¸ ë° ì´ë™
            try:
                if not has_next_page_group(driver):
                    break
            except KeyboardInterrupt:
                print("\nìˆ˜ì§‘ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                raise
        
        # DataFrame ë³€í™˜ ë° ë°˜í™˜
        if all_reviews:
            reviews_df = pd.DataFrame(all_reviews)
            print(f"  -> ì´ {len(reviews_df)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return reviews_df
        else:
            print("  -> ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame(columns=['ProductID', 'Date', 'Review'])
            
    except Exception as e:
        print(f"ìƒí’ˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame(columns=['ProductID', 'Date', 'Review'])


def generate_output_filename(product_id):
    """ì¶œë ¥ íŒŒì¼ëª… ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"Coupang_Reviews_{product_id}_{timestamp}.csv"


def save_reviews_to_csv(reviews_df, output_filename):
    """ë¦¬ë·° ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    reviews_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
    print(f"ë¦¬ë·° ë°ì´í„°ê°€ '{output_filename}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")




def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹"""
    print("="*60)
    print("ğŸ›ï¸ ì¿ íŒ¡ ë¦¬ë·° ìˆ˜ì§‘ê¸° (ë°°ì¹˜ ì²˜ë¦¬)")
    print("="*60)
    
    try:
        # ì‚¬ìš©ë²• ì•ˆë‚´
        if len(sys.argv) == 1:
            print("ì‚¬ìš©ë²•:")
            print("  ë‹¨ì¼ URL ì²˜ë¦¬: python get_reviews.py <product_url>")
            print("  ë°°ì¹˜ ì²˜ë¦¬:     python get_reviews.py <url_list_file.txt>")
            print("\nì˜ˆì‹œ:")
            print("  python get_reviews.py https://www.coupang.com/vp/products/123456789")
            print("  python get_reviews.py coupang_list.txt")
            return
        
        # ì²« ë²ˆì§¸ ì¸ì ë¶„ì„
        first_arg = sys.argv[1]
        
        if "coupang.com" in first_arg:
            # ë‹¨ì¼ URL ì²˜ë¦¬ ëª¨ë“œ
            clean_url = first_arg.split('?')[0]
            product_urls = [clean_url]
            print(f"ë‹¨ì¼ ìƒí’ˆ ì²˜ë¦¬ ëª¨ë“œ")
            print(f"ëŒ€ìƒ ìƒí’ˆ: {clean_url}")
        else:
            # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ (í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì½ê¸°)
            list_file = first_arg
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ - {list_file}ì—ì„œ URL ëª©ë¡ì„ ì½ìŠµë‹ˆë‹¤...")
            product_urls = read_url_list(list_file)
            
            if not product_urls:
                print("ì²˜ë¦¬í•  URLì´ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
            
            print(f"ì´ {len(product_urls)}ê°œì˜ ìƒí’ˆì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        print("="*60)
        
        # Chrome ë“œë¼ì´ë²„ í•œ ë²ˆë§Œ ì´ˆê¸°í™”
        driver = None
        success_count = 0
        fail_count = 0
        total_reviews = 0
        
        try:
            print("Chrome ë“œë¼ì´ë²„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
            driver = create_chrome_driver()
            print("âœ… ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ!\n")
            
            # ê° ìƒí’ˆ ìˆœì°¨ ì²˜ë¦¬
            for index, product_url in enumerate(product_urls, 1):
                try:
                    # ìƒí’ˆë³„ ë¦¬ë·° ìˆ˜ì§‘
                    reviews_df = process_single_product(product_url, index, len(product_urls), driver)
                    
                    if not reviews_df.empty:
                        # CSV íŒŒì¼ë¡œ ì €ì¥
                        product_id = reviews_df.iloc[0]['ProductID']
                        output_filename = generate_output_filename(product_id)
                        save_reviews_to_csv(reviews_df, output_filename)
                        
                        success_count += 1
                        total_reviews += len(reviews_df)
                        print(f"  âœ… ì„±ê³µ: {len(reviews_df)}ê°œ ë¦¬ë·° â†’ {output_filename}")
                    else:
                        fail_count += 1
                        print(f"  âŒ ì‹¤íŒ¨: ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ë‹¤ìŒ ìƒí’ˆìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ì „ ì ì‹œ ëŒ€ê¸°
                    if index < len(product_urls):
                        print("  ğŸ’¤ ë‹¤ìŒ ìƒí’ˆ ì²˜ë¦¬ë¥¼ ìœ„í•´ 3ì´ˆ ëŒ€ê¸° ì¤‘...\n")
                        time.sleep(3)
                        
                except KeyboardInterrupt:
                    print(f"\nì‚¬ìš©ìì— ì˜í•´ ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
                except Exception as e:
                    fail_count += 1
                    print(f"  âŒ ìƒí’ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\n")
                    continue
            
        finally:
            # ë“œë¼ì´ë²„ ì¢…ë£Œ
            safe_driver_quit(driver)
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print("="*60)
        print("ğŸ‰ ë°°ì¹˜ ì‘ì—… ì™„ë£Œ!")
        print("="*60)
        print("ğŸ“Š ì‘ì—… ê²°ê³¼ ìš”ì•½:")
        print(f"   - ì´ ì²˜ë¦¬ ëŒ€ìƒ: {len(product_urls)}ê°œ")
        print(f"   - ì„±ê³µ: {success_count}ê°œ")
        print(f"   - ì‹¤íŒ¨: {fail_count}ê°œ")
        print(f"   - ì´ ìˆ˜ì§‘ ë¦¬ë·°: {total_reviews:,}ê°œ")
        print(f"   - ì„±ê³µë¥ : {(success_count/len(product_urls)*100):.1f}%")
        
        if fail_count > 0:
            print(f"\nâš ï¸  ì¼ë¶€ ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
    except KeyboardInterrupt:
        print("\nì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()